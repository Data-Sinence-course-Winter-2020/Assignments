{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1 - FINAL EDITION 9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP7dzsw0A/QWFdZ11vZZuAw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Data-Sinence-course-Winter-2020/Projects/blob/master/Assignment_1_FINAL_EDITION_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8PQe1xIIfRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Final Version of Classification /// Assignment 1\n",
        "#Define Libraries\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "nltk.download(\"gutenberg\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "set(stopwords.words(\"english\"))\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from random import seed\n",
        "from random import randint\n",
        "from random import shuffle\n",
        "import array\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "import sklearn.datasets \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn import tree\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "import glob\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#########################\n",
        "#define list of possible books from gutenberg\n",
        "nltk.corpus.gutenberg.fileids() \n",
        "[\"austen-emma.txt\", \"austen-persuasion.txt\", \"austen-sense.txt\", \"bible-kjv.txt\",\n",
        "\"blake-poems.txt\", \"bryant-stories.txt\", \"burgess-busterbrown.txt\",\n",
        "\"carroll-alice.txt\", \"chesterton-ball.txt\", \"chesterton-brown.txt\",\n",
        "\"chesterton-thursday.txt\", \"edgeworth-parents.txt\", \"melville-moby_dick.txt\",\n",
        "\"milton-paradise.txt\", \"shakespeare-caesar.txt\", \"shakespeare-hamlet.txt\",\n",
        "\"shakespeare-macbeth.txt\", \"whitman-leaves.txt\"]\n",
        "#########################\n",
        "\n",
        "\n",
        "#########################\n",
        "#choosing 7 books in list in lower case\n",
        "#origin is the origin version of chosen books\n",
        "origin = [nltk.corpus.gutenberg.raw(\"austen-emma.txt\"), nltk.corpus.gutenberg.raw(\"milton-paradise.txt\"),\n",
        "          nltk.corpus.gutenberg.raw(\"chesterton-ball.txt\") , nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\") ,\n",
        "          nltk.corpus.gutenberg.raw(\"edgeworth-parents.txt\") , nltk.corpus.gutenberg.raw(\"bryant-stories.txt\") ,\n",
        "          nltk.corpus.gutenberg.raw(\"whitman-leaves.txt\") ]\n",
        "books = [nltk.corpus.gutenberg.raw(\"austen-emma.txt\").lower(), nltk.corpus.gutenberg.raw(\"milton-paradise.txt\").lower(),\n",
        "         nltk.corpus.gutenberg.raw(\"chesterton-ball.txt\").lower() , nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\").lower(),\n",
        "         nltk.corpus.gutenberg.raw(\"edgeworth-parents.txt\").lower() , nltk.corpus.gutenberg.raw(\"bryant-stories.txt\").lower() , \n",
        "         nltk.corpus.gutenberg.raw(\"whitman-leaves.txt\").lower() ]\n",
        "#########################\n",
        "\n",
        "\n",
        "#########################\n",
        "#extract words from string without punctuation\n",
        "def book_punc(i):\n",
        "  books[i] = re.sub(\"[\"+string.punctuation+\"]\", \"\", str(books[i])).split()\n",
        "#########################\n",
        "  \n",
        "  \n",
        "#########################  \n",
        "#removing stop words \n",
        "def book_stopwords(i):\n",
        "  tokenized_words = books[i] \n",
        "  stop_words = stopwords.words(\"english\")\n",
        "  books[i]=[word for word in tokenized_words if word not in stop_words]\n",
        "#########################\n",
        "\n",
        "\n",
        "#########################  \n",
        "#Stemmer \n",
        "def book_stemmer(i):\n",
        "  ps = PorterStemmer()\n",
        "  example_words = books[i]\n",
        "  new = []\n",
        "  for w in example_words:\n",
        "   new.append(ps.stem(w))\n",
        "  books[i]=new\n",
        "#########################\n",
        "  \n",
        "  \n",
        "#########################\n",
        "#number of words in each document\n",
        "def book_docWords(i):\n",
        "  documentW=len(books[i]) // 200   #counting the number of words per documents ?\n",
        "  return int(documentW)\n",
        "#########################\n",
        "  \n",
        "\n",
        "#########################\n",
        "#make 200 random numbers for as a pointer to each word of a document.\n",
        "#The goal is start reading from the pointer as a random number \n",
        "def book_randNum (documentWo,i):\n",
        "  documentW = documentWo\n",
        "  value = []\n",
        "  #Preventing possible errors if each section of a book had less than 150 words\n",
        "  if (documentW > 80):\n",
        "    tguess = documentW - 80\n",
        "    for i in range (200): #should be  200\n",
        "      tmp=randint(0,tguess)\n",
        "      value.append(tmp)\n",
        "  else:\n",
        "    tguess = documentW\n",
        "    for i in range (200): #should be 200\n",
        "      tmp=randint(0,tguess)\n",
        "      if (i == 199):  # should be 200 - 1\n",
        "        tmp=tmp-documentW\n",
        "      value.append(tmp)\n",
        "  return value\n",
        "#########################\n",
        "  \n",
        "\n",
        "#########################\n",
        "#make a list of 200 sublist *  contains 150 words (for each book, obviously!)\n",
        "def book_listWord (documentWs ,values , i):\n",
        "  value = values\n",
        "  documentW = documentWs\n",
        "  \n",
        "  dataset = []\n",
        "  datasetF = []\n",
        "\n",
        "  res=books[i]\n",
        "  for i in range (200):  # after testing should be 200 ~ number of sample documents for each book\n",
        "    point = (i * documentW) + value[i]\n",
        "    for x in range (80): #should be 150 ~ number of words in each sample\n",
        "      dataset.append(res[point])\n",
        "      point += 1\n",
        "      #print(dataset) #check each line\n",
        "      if len(dataset) == 80 :  #should be 150 ~ appending words to the dataset\n",
        "        datasetF.append((list(dataset)))\n",
        "        dataset.clear()      \n",
        "        \n",
        "  return list (datasetF)\n",
        "#########################\n",
        "\n",
        "\n",
        "#########################\n",
        "#MAKE DATA FRAME FROM FINAL DATASET (which a list containst 7 list , 200 sublist and each sublist contains 150 words) ~\n",
        "#### this data is for the whole books \n",
        "def book_lable(finList):\n",
        "  booknamelist = [\"Emma\", \"Paradise\", \"Ball\", \"Moby\", \"Parents\" , \"Stories\" ,  \"Leaves\"]\n",
        "  authornamelist = [\"Austun\", \"Milton\", \"Chesterton\", \"Melville\", \"Edgeworth\" , \"Briant\" , \"Whitman\"]\n",
        "  column_names = [\"BookName\" , \"AuthorName\" , \"Content\"]\n",
        "  df2 = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  for j in range (7): #always be 7 (7 list)\n",
        "    for k in range (200):  #should be 200 after tests\n",
        "      datas = { \n",
        "      \"BookName\": booknamelist[j] , \"AuthorName\": authornamelist[j] , \"Content\": (finList[j][k:k+1 ])\n",
        "      #from finlist[j][k] to finlist[j][k+1]\n",
        "         }\n",
        "      df = pd.DataFrame.from_dict(datas)\n",
        "      frames = [df]\n",
        "      result = pd.concat(frames)\n",
        "      df2 = df2.append(result)\n",
        "\n",
        "  return df2\n",
        "#########################\n",
        "\n",
        "\n",
        "#########################\n",
        "#The main function\n",
        "bookGlist = []\n",
        "bookFlist = []\n",
        "values = []\n",
        "column_names = [\"BookName\" , \"AuthorName\" , \"Content\"]\n",
        "dataFrame_final = pd.DataFrame(columns = column_names)\n",
        "for i in range(7):\n",
        "  book_punc(i)\n",
        "  book_stopwords(i)\n",
        "  book_stemmer(i)\n",
        "  documentW = book_docWords(i)\n",
        "  values = book_randNum (documentW , i)\n",
        "  bookGlist = book_listWord(documentW , values , i)\n",
        "  bookFlist.append(bookGlist)\n",
        "#########################\n",
        "  \n",
        "  \n",
        "######################### \n",
        "print(\"DONE :: End of books process\")\n",
        "#########################\n",
        "\n",
        "\n",
        "#########################\n",
        "dataFrame_final = book_lable(bookFlist)  #labling books\n",
        "dataFrame_final = dataFrame_final.reset_index() #make index - for the shuffling\n",
        "dataFrame_finalshuff = dataFrame_final.reindex(np.random.permutation(dataFrame_final.index))  #shuffle dataframe\n",
        "#########################\n",
        "\n",
        "\n",
        "#########################\n",
        "#Joint for dataFrame Content\n",
        "dataFrame = pd.DataFrame(index=range(1400), columns=[\"Content\", \"AuthorName\"])\n",
        "for i in range(1400):\n",
        "    dataFrame.at[i,\"Content\"] = \" \".join(dataFrame_final.at[i, \"Content\"])\n",
        "    dataFrame.at[i,\"AuthorName\"] = dataFrame_final.at[i,\"AuthorName\"]\n",
        "    dataFrame = dataFrame.reindex(np.random.permutation(dataFrame.index)) \n",
        "#########################\n",
        "\n",
        "\n",
        "########################\n",
        "print(\"\\n\\n\",\"::::::>>>>>>> NAIVE <<<<<<<::::::\")\n",
        "#Naive + Vectorizer\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataFrame[\"Content\"], dataFrame[\"AuthorName\"],\n",
        "                                              test_size=0.2,     random_state = 1)\n",
        "count_vect = CountVectorizer()\n",
        "fiter = count_vect.fit(X_train)\n",
        "X_train_counts = fiter.transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "fiter2 = tfidf_transformer.fit(X_train_counts)\n",
        "X_train_tfidf = fiter2.transform(X_train_counts)\n",
        "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\">>> Naive Score:\")\n",
        "print(clf.score(fiter2.transform(fiter.transform(X_test)),y_test))\n",
        "\n",
        "#Cross Validation\n",
        "\n",
        "X=dataFrame['Content']\n",
        "y=dataFrame['AuthorName']\n",
        "scores1 = cross_val_score(clf, fiter2.transform(fiter.transform(X)),y, cv=10)\n",
        "print(\">>> Cross Validation Score:\")\n",
        "print(scores1)\n",
        "########################\n",
        "\n",
        "\n",
        "########################\n",
        "#KNN\n",
        "print(\"\\n\\n\",\"::::::>>>>>>> KNN <<<<<<<::::::\")\n",
        "knn = KNeighborsClassifier(n_neighbors=7) \n",
        "knn.fit(X_train_tfidf, y_train) \n",
        "X_testf=fiter2.transform(fiter.transform(X_test))\n",
        "predict_knn = knn.predict(fiter2.transform(fiter.transform(X_test)))\n",
        "y_pred=knn.predict(X_testf)\n",
        "\n",
        "# Calculate the accuracy of the model \n",
        "print(\">>> KNN Score:\")\n",
        "accuracy1 = accuracy_score(y_test, y_pred)\n",
        "print(accuracy1)\n",
        "\n",
        "#KNN - 10 Fold - Confusion Mattrix\n",
        "\n",
        "print(\">>> KNN Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "scores2 = cross_val_score(knn, fiter2.transform(fiter.transform(X)) , y , cv=10)\n",
        "print(\">>> 10 Fold Cross Validation - KNN - Score:\")\n",
        "print (scores2)\n",
        "######################\n",
        "\n",
        "\n",
        "######################\n",
        "print(\"\\n\\n\",\"::::::>>>>>>> SVM <<<<<<<::::::\")\n",
        "\n",
        "classifier = svm.SVC(kernel='linear', C=1)\n",
        "\n",
        "# Train it on the entire training data set\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get predictions on the test set\n",
        "y_pred = classifier.predict(X_testf)\n",
        "print(\">>> SVM Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "classifier.score(fiter2.transform(fiter.transform(X_test)),y_test)\n",
        "scores3 = cross_val_score(classifier, fiter2.transform(fiter.transform(X)),y, cv=10)\n",
        "print(\">>> 10 Fold Cross Validation - SVM - Score:\")\n",
        "print (scores3)\n",
        "\n",
        "# Calculate the accuracy of the model \n",
        "print(\">>> SVM Score:\")\n",
        "accuracy2 = accuracy_score(y_test, y_pred)\n",
        "print(accuracy2)\n",
        "######################\n",
        "\n",
        "\n",
        "######################\n",
        "# Decision Tree and 10-fold cross validation\n",
        "print(\"\\n\\n\",\"::::::>>>>>>> Decision Tree <<<<<<<::::::\")\n",
        "\n",
        "\n",
        "DT = DecisionTreeClassifier()\n",
        "DT.fit(X_train_tfidf, y_train)\n",
        "plt.figure()\n",
        "plot_tree(DT, filled=True)\n",
        "print(\">>> Decision Tree:\")\n",
        "plt.show()\n",
        "y_pred = DT.predict(X_testf)\n",
        "print(\">>> Decision Tree Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\">>> Decision Tree Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\">>> Decision Tree Score: \\n\" , metrics.accuracy_score(y_test, y_pred))\n",
        "print(\">>> 10 Fold Cross Validation - Decision Tree - Score:\")\n",
        "scores4 = cross_val_score(DT, fiter2.transform(fiter.transform(X)),y, cv=10)\n",
        "print(scores4)\n",
        "######################\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}