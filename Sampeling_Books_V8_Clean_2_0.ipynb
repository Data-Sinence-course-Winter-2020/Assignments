{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sampeling Books V8 - Clean 2.0",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnvhSOYc5FW1t/avb5JbHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Data-Sinence-course-Winter-2020/Assignments/blob/master/Sampeling_Books_V8_Clean_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhN5S8PBkYfU",
        "colab_type": "code",
        "outputId": "c06e9921-cdce-49eb-b489-ca86b6f3738b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#V8 - Clean code 2.0 *\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from random import seed\n",
        "from random import randint\n",
        "from random import shuffle\n",
        "import array\n",
        "import numpy as np \n",
        "import pandas as pd "
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSpLmeko3E-s",
        "colab_type": "code",
        "outputId": "3975b62f-1a06-4cdc-fbcd-0d9320f8dea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "nltk.corpus.gutenberg.fileids() #define list of possible books from gutenberg\n",
        "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',\n",
        "'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt',\n",
        "'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',\n",
        "'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n",
        "'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt',\n",
        "'shakespeare-macbeth.txt', 'whitman-leaves.txt']"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T5XtZ8f3k4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#choosing 7 books in list in lower case\n",
        "#origin is the origin veriosn of chosen books\n",
        "origin = [nltk.corpus.gutenberg.raw('austen-emma.txt'), nltk.corpus.gutenberg.raw('milton-paradise.txt'), nltk.corpus.gutenberg.raw('chesterton-ball.txt') , nltk.corpus.gutenberg.raw('melville-moby_dick.txt') , nltk.corpus.gutenberg.raw('edgeworth-parents.txt') , nltk.corpus.gutenberg.raw('bryant-stories.txt') , nltk.corpus.gutenberg.raw('whitman-leaves.txt') ]\n",
        "books = [nltk.corpus.gutenberg.raw('austen-emma.txt').lower(), nltk.corpus.gutenberg.raw('milton-paradise.txt').lower(), nltk.corpus.gutenberg.raw('chesterton-ball.txt').lower() , nltk.corpus.gutenberg.raw('melville-moby_dick.txt').lower() , nltk.corpus.gutenberg.raw('edgeworth-parents.txt').lower() , nltk.corpus.gutenberg.raw('bryant-stories.txt').lower() , nltk.corpus.gutenberg.raw('whitman-leaves.txt').lower() ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFzuXy4HDtzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract words from string without punctuation\n",
        "def book_punc(i):\n",
        "  books[i] = re.sub('['+string.punctuation+']', '', str(books[i])).split() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cGONczTUSA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removing stop words \n",
        "def book_stopwords(i):\n",
        "  tokenized_words = books[i] \n",
        "  stop_words = stopwords.words('english')\n",
        "  books[i]=[word for word in tokenized_words if word not in stop_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwes96wPUqS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stemmer \n",
        "def book_stemmer(i):\n",
        "  ps = PorterStemmer()\n",
        "  example_words = books[i]\n",
        "  new = []\n",
        "  for w in example_words:\n",
        "   new.append(ps.stem(w))\n",
        "  books[i]=new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkw5vTMRsFCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of words in each document\n",
        "def book_docWords(i):\n",
        "  documentW=len(books[i]) // 200   #number of words per documents\n",
        "  return int(documentW)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4eZpJKljo-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make 200 random numbers for as a pointer to each word of a document. The goal is start reading from the pointer as a random number \n",
        "def book_randNum (documentWo,i):\n",
        "  documentW = documentWo\n",
        "  value = []\n",
        "  #Preventing possible errors if each section of a book had less than 150 words\n",
        "  if (documentW > 150):\n",
        "    tguess = documentW - 150\n",
        "    for i in range (200): #should be  200\n",
        "      tmp=randint(0,tguess)\n",
        "      value.append(tmp)\n",
        "  else:\n",
        "    tguess = documentW\n",
        "    for i in range (200): #should be 200\n",
        "      tmp=randint(0,tguess)\n",
        "      if (i == 199):  # should be 200 - 1\n",
        "        tmp=tmp-documentW\n",
        "      value.append(tmp)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VZDc5d8ajhc6",
        "colab": {}
      },
      "source": [
        "#make a list of 200 sublist *  contains 150 words (for each book, obviously!)\n",
        "def book_listWord (documentWs ,values , i):\n",
        "  value = values\n",
        "  documentW = documentWs\n",
        "  \n",
        "  dataset = []\n",
        "  datasetF = []\n",
        "  listoflists = []\n",
        "  a_list = []\n",
        "  lstF = []\n",
        "\n",
        "  res=books[i]\n",
        "  for i in range (200):  # after testing should be 200 ~ number of sample documents for each book\n",
        "    point = (i * documentW) + value[i]\n",
        "    for x in range (150): #should be 150 ~ number of words in each sample\n",
        "      dataset.append(res[point])\n",
        "      point += 1\n",
        "      #print(dataset) #check each line\n",
        "      if len(dataset) == 150 :  #should be 150 ~ appending words to the dataset\n",
        "        datasetF.append((list(dataset)))\n",
        "        dataset.clear()      \n",
        "        \n",
        "  return list (datasetF)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URXh0KGWlgMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MAKE DATA FRAME FROM FINAL DATASET (which a list containst 7 list , 200 sublist and each sublist contains 150 words) ~ this data is for the whole books \n",
        "\n",
        "def book_lable(finList):\n",
        "  lst = finList\n",
        "  dct = {}\n",
        "  booknamelist = ['Emma', 'Paradise', 'Ball', 'Moby', 'Parents' , 'Stories' ,  'Leaves']\n",
        "  authornamelist = ['Austun', 'Milton', 'Chesterton', 'Melville', 'Edgeworth' , 'Briant' , 'Whitman']\n",
        "  column_names = [\"BookName\" , \"AuthorName\" , \"Content\"]\n",
        "  df2 = pd.DataFrame(columns = column_names)\n",
        "\n",
        "  for j in range (7): #always be 7 (7 list)\n",
        "    for k in range (200):  #should be 200 after tests\n",
        "      datas = { \n",
        "      \"BookName\": booknamelist[j] , \"AuthorName\": authornamelist[j] , \"Content\": (finList[j][k:k+1 ]) #from finlist[j][k] to finlist[j][k+1]\n",
        "         }\n",
        "      df = pd.DataFrame.from_dict(datas)\n",
        "      frames = [df]\n",
        "      result = pd.concat(frames)\n",
        "      df2 = df2.append(result)\n",
        "\n",
        "  return df2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "077wVVOGtwIj",
        "colab_type": "code",
        "outputId": "1562dfb0-d8a2-48f9-c991-c14a49c6596f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#The main function\n",
        "bookGlist = []\n",
        "bookFlist = []\n",
        "values = []\n",
        "\n",
        "column_names = [\"BookName\" , \"AuthorName\" , \"Content\"]\n",
        "dataFrame_final = pd.DataFrame(columns = column_names)\n",
        "for i in range(7):\n",
        "  book_punc(i)\n",
        "  book_stopwords(i)\n",
        "  book_stemmer(i)\n",
        "  documentW = book_docWords(i)\n",
        "  values = book_randNum (documentW , i)\n",
        "  bookGlist = book_listWord(documentW , values , i)\n",
        "  bookFlist.append(bookGlist)\n",
        "\n",
        "\n",
        "print(\"DONE :: End of books process\")\n",
        "#print(bookFlist) #Lists ins order without lable (Just for TEST)\n",
        "\n",
        "dataFrame_final = book_lable(bookFlist)  #labling books\n",
        "dataFrame_final = dataFrame_final.reset_index() #make index - for the shuffling\n",
        "dataFrame_finalshuff = dataFrame_final.reindex(np.random.permutation(dataFrame_final.index))  #This is the final target which has shuffled\n",
        "\n",
        "print(\"list of shuffled Dataaform\")\n",
        "#print (dataFrame_final[['BookName', 'AuthorName' , 'Content']]) #dataFrame without shuffling for test\n",
        "print (dataFrame_finalshuff[['BookName', 'AuthorName' , 'Content']])\n",
        "#>>> dataFrame_finalshuff can be implemented for the rest of the code <<<\n"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE :: End of books process\n",
            "list of shuffled Dataaform\n",
            "      BookName  AuthorName                                            Content\n",
            "433       Ball  Chesterton  [happen, badli, happen, success, happen, unsuc...\n",
            "1118   Stories      Briant  [meant, great, deal, work, knew, well, old, gr...\n",
            "856    Parents   Edgeworth  [gardenwicket, stay, first, inform, susan, ros...\n",
            "304   Paradise      Milton  [heaven, heaven, presum, earthli, guest, drawn...\n",
            "645       Moby    Melville  [warbl, persua, pleasant, holiday, weather, ca...\n",
            "...        ...         ...                                                ...\n",
            "113       Emma      Austun  [use, music, societi, mapl, grove, bath, would...\n",
            "27        Emma      Austun  [perri, remonstr, told, shabbi, best, player, ...\n",
            "1100   Stories      Briant  [fli, toward, snowwhit, sun, great, wing, beat...\n",
            "138       Emma      Austun  [news, last, night, ball, seem, lost, gipsi, p...\n",
            "506       Ball  Chesterton  [handsom, figur, curl, yellow, hair, lean, fas...\n",
            "\n",
            "[1400 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}